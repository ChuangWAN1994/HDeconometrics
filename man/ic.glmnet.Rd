% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ic.glmnet.R
\name{ic.glmnet}
\alias{ic.glmnet}
\title{Estimate a GLM with lasso, elasticnet or ridge regularization using the information criterion}
\usage{
ic.glmnet(x, y, crit = c("bic", "aic", "aicc", "hqc"), ...)
}
\arguments{
\item{x}{Matrix of independent variables. Each row is an observation and each column is a variable.}

\item{y}{Response variable equivalent to the function.}

\item{crit}{Information criterion.}

\item{...}{Aditional arguments to be passed to glmnet.}
}
\value{
An object with S3 class ic.glmnet.
\item{coefficients}{Coefficients from the selected model.}
\item{ic}{All information criterions.}
\item{lambda}{Lambda from the selected model.}
\item{nvar}{Number of variables on the selected model including the intercept.}
\item{glmnet}{glmnet object.}
\item{residuals}{Residuals from the selected model.}
\item{fitted.values}{Fitted values from the selected model.}
\item{ic.range}{Chosen information criterion calculated through all the regularization path.}
\item{call}{The matched call.}
}
\description{
Uses the glmnet function from the glmnet package to estimate models through all the regularization path and selects the best model using some information criterion. The glmnet package chooses the best model only by cross validation (cv.glmnet). Choosing with information criterion is faster and more adequate for some aplications, especially time-series.
}
\details{
Selecting the model using information criterion is faster than using cross validation and it has some theoretical advantages in some cases. For example, Zou, Hastie, Tibshirani (2007) show that one can consistently estimate the degrees of freedom of the LASSO using the BIC. Moreover, Information Criterions are becoming very popular, especially on time-series applications where cross-validation may impose further complications.

The information criterions implmemented are the Bayesian Information Criterion (bic), the Akaike Information Criterion (aic) and its sample size correction (aicc) and the Hannah and Quinn Criterion (hqc).
}
\examples{
## == This example uses the Brazilian inflation data from
#Garcia, Medeiros and Vasconcelos (2017) == ##
data("BRinf")

## == Data preparation == ##
## == The model is yt = a + Xt-1'b + ut == ##
aux = embed(BRinf,2)
y=aux[,1]
x=aux[,-c(1:ncol(BRinf))]

## == Ridge == ##
ridge=ic.glmnet(x,y,crit = "bic",alpha=0)
coef(ridge)


## == LASSO == ##
lasso=ic.glmnet(x,y,crit = "bic")
coef(lasso)
fitted(lasso)
residuals(lasso)
lasso$ic

## == Adaptive LASSO == ##
tau=1
# Lasso as the first step model, intercept must be removed
# to calculate the penalty factor.
first.step.coef=coef(lasso)[-1]
penalty.factor=abs(first.step.coef+1/sqrt(nrow(x)))^(-tau)
adalasso=ic.glmnet(x,y,crit="bic",penalty.factor=penalty.factor)
coef(adalasso)
adalasso$ic

}
\references{
Garcia, Medeiros and Vasconcelos (2017).

Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL \url{http://www.jstatsoft.org/v33/i01/}.

Zou, Hui, Trevor Hastie, and Robert Tibshirani. "On the “degrees of freedom” of the lasso." The Annals of Statistics 35.5 (2007): 2173-2192.
}
\seealso{
\code{\link[glmnet]{glmnet}}, \code{\link[glmnet]{cv.glmnet}}, \code{\link{predict}}, \code{\link{plot}}
}
\keyword{Elasticnet,}
\keyword{LASSO,}
\keyword{Regularization}
\keyword{Ridge,}
\keyword{glmnet,}
